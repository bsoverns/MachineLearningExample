<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.ML.Tokenizers</name>
    </assembly>
    <members>
        <member name="T:Microsoft.ML.Tokenizers.AddedToken">
            <summary>
            Represent a token added by the user on top of the existing Model vocabulary.
            AddedToken can be configured to specify the behavior they should have in various situations
            like:
              - Whether they should only match single words
              - Whether to include any WhiteSpace on its left or right
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.AddedToken.Content">
            <summary>
            Gets or sets the content of the added token
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.AddedToken.SingleWord">
            <summary>
            Gets or sets whether this token must be a single word or can break words
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.AddedToken.LeftStrip">
            <summary>
            Gets or sets whether this token should strip WhiteSpaces on its left
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.AddedToken.RightStrip">
            <summary>
            Gets or sets whether this token should strip WhiteSpaces on its right
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.AddedToken.Normalized">
            <summary>
            Gets or sets whether this token should be normalized
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.AddedToken.Special">
            <summary>
            Gets or sets whether this token is special
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.AddedToken.#ctor">
            <summary>
            Create a new AddedToken object.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.AddedToken.#ctor(System.String,System.Boolean)">
            <summary>
            Create a new AddedToken object from the given content, specifying if it is intended to be a
            special token. Special tokens are not normalized by default.
            </summary>
            <param name="content">The content of the added token.</param>
            <param name="special">Indicate whether this token is special.</param>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.AddedToken.Equals(Microsoft.ML.Tokenizers.AddedToken)">
            <summary>
            Determines whether two token instances are equal.
            </summary>
            <param name="other">The token to compare with the current token.</param>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.AddedToken.GetHashCode">
            <summary>
            Returns the hash code for the current token.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.AddedToken.op_Implicit(System.String)~Microsoft.ML.Tokenizers.AddedToken">
            <summary>
            Defines an implicit conversion of a string object to AddedToken.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.Bpe">
            <summary>
            Represent the Byte Pair Encoding model.
            </summary>
        </member>
        <member name="F:Microsoft.ML.Tokenizers.Bpe._unknownToken">
            A [Byte Pair Encoding](https://www.aclweb.org/anthology/P16-1162/) model.
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Bpe.UnknownToken">
            <summary>
            Gets or Sets unknown token. The unknown token to be used when we encounter an unknown char
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Bpe.ContinuingSubwordPrefix">
            <summary>
            An optional prefix to use on any sub-word that exist only behind another one
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Bpe.EndOfWordSuffix">
            <summary>
            An optional suffix to characterize and end-of-word sub-word
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Bpe.FuseUnknownTokens">
            <summary>
            Gets or sets whether allowing multiple unknown tokens get fused
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Bpe.#ctor">
            <summary>
            Construct a new Bpe model object with no tokenization vocabulary. This constructor is useful only in the training scenario.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Bpe.#ctor(System.String,System.String,System.String,System.String,System.String)">
            <summary>
            Construct a new Bpe model object to use for sentence tokenization and tokenizer training.
            </summary>
            <param name="vocabFile">The JSON file path containing the dictionary of string keys and their ids.</param>
            <param name="mergesFile">The file path containing the tokens's pairs list.</param>
            <param name="unknownToken"> The unknown token to be used by the model.</param>
            <param name="continuingSubwordPrefix">The prefix to attach to sub-word units that don’t represent a beginning of word.</param>
            <param name="endOfWordSuffix">The suffix to attach to sub-word units that represent an end of word.</param>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Bpe.Decoder">
            <summary>
            Gets the Bpe decoder object.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Bpe.Tokenize(System.String)">
            <summary>
            Tokenize a sequence string to a list of tokens.
            </summary>
            <param name="sequence">The sequence to tokenize.</param>
            <returns>The list of tokens generated from the sequence tokenization.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Bpe.TokenToId(System.String)">
            <summary>
            Map the token to tokenized Id.
            </summary>
            <param name="token">The token to map to the Id.</param>
            <returns>The mapped Id of the token.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Bpe.IdToToken(System.Int32,System.Boolean)">
            <summary>
            Map the tokenized Id to the token.
            </summary>
            <param name="id">The Id to map to the token.</param>
            <param name="skipSpecialTokens">Indicate if want to skip the special tokens during the decoding.</param>
            <returns>The mapped token of the Id.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Bpe.GetVocab">
            <summary>
            Gets the dictionary mapping tokens to Ids.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Bpe.GetVocabSize">
            <summary>
            Gets the dictionary size that map tokens to Ids.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Bpe.GetTrainer">
            <summary>
            Gets a trainer object to use in training the model and generate the vocabulary and merges data.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Bpe.Save(System.String,System.String)">
            <summary>
            Save the model data into the vocabulary and merges files.
            </summary>
            <param name="path">The file system path to store the generated files at.</param>
            <param name="prefix">Optional prefix for the generated file names.</param>
            <returns>The list of all saved files.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Bpe.ReadFile(System.String,System.String)">
            Read the given files to extract the vocab and merges
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Bpe.Vocab">
            The vocabulary assigns a number to each token.
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Bpe.Merges">
            Contains the mapping between Pairs and their (rank, newId).
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Bpe.Cache">
            Contains the cache for optimizing the encoding step.
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Bpe.VocabReverse">
            Reversed vocabulary, to rebuild sentences.
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Bpe.Dropout">
            Dropout probability for merges. 0 = no dropout is the default. At 1.0, tokenization will
            perform no merges, so the result will just be characters.
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Bpe.ConvertMergesToHashmap(System.String)">
            Converts the merges strings (for example from `merges.txt` file) with the format
            "{pair_a} {pair_b}" into the format expected by the BPE struct
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Bpe.ClearCache">
            Reset the cache.
        </member>
        <member name="T:Microsoft.ML.Tokenizers.BpeDecoder">
            <summary>
            Allows decoding Original BPE by joining all the tokens and then replacing
            the suffix used to identify end-of-words by white spaces
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.BpeDecoder.#ctor">
            <summary>
            Construct a new Bpe decoder object.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.BpeDecoder.#ctor(System.String)">
            <summary>
            Construct a new Bpe decoder object.
            </summary>
            <param name="suffix">The suffix that was used to characterize an end-of-word. This suffix will be replaced by white spaces during the decoding.</param>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.BpeDecoder.Decode(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Decode the original BPE by joining all the tokens and then replacing the suffix used to identify end-of-words by white spaces.
            </summary>
            <param name="tokens">The list of tokens to merge.</param>
            <returns>The string containing all merged tokens.</returns>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.BpeTrainer">
            <summary>
            The Bpe trainer responsible to train the Bpe model.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.BpeTrainer.VocabSize">
            <summary>
            Gets the size of the final vocabulary, including all tokens and alphabet.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.BpeTrainer.MinFrequency">
            <summary>
            Gets the minimum frequency a pair should have in order to be merged.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.BpeTrainer.SpecialTokens">
            <summary>
            Gets the list of special tokens the model should know of.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.BpeTrainer.LimitAlphabet">
            <summary>
            Gets the maximum different characters to keep in the alphabet.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.BpeTrainer.InitialAlphabet">
            <summary>
            Gets the list of characters to include in the initial alphabet, even if not seen in the training dataset.
            If the strings contain more than one character, only the first one is kept.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.BpeTrainer.ContinuingSubwordPrefix">
            <summary>
            Gets the prefix to be used for every sub-word that is not a beginning-of-word.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.BpeTrainer.EndOfWordSuffix">
            <summary>
            Gets the suffix to be used for every sub-word that is a end-of-word.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.BpeTrainer.#ctor">
            <summary>
            Construct a new BpeTrainer object using the default values.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.BpeTrainer.#ctor(System.Collections.Generic.IEnumerable{Microsoft.ML.Tokenizers.AddedToken},System.Int32,System.Int32,Microsoft.ML.Tokenizers.ReportProgress,System.Nullable{System.Int32},System.Collections.Generic.HashSet{System.Char},System.String,System.String)">
            <summary>
            Construct a new BpeTrainer object.
            </summary>
            <param name="specialTokens">The list of special tokens the model should know of.</param>
            <param name="minFrequency">The minimum frequency a pair should have in order to be merged.</param>
            <param name="vocabSize">the size of the final vocabulary, including all tokens and alphabet.</param>
            <param name="progress">Callback for the training progress updates.</param>
            <param name="limitAlphabet">The list of characters to include in the initial alphabet.</param>
            <param name="initialAlphabet">The JSON file path containing the dictionary of string keys and their ids</param>
            <param name="continuingSubwordPrefix">the prefix to be used for every sub-word that is not a beginning-of-word.</param>
            <param name="endOfWordSuffix">the suffix to be used for every sub-word that is a end-of-word.</param>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.BpeTrainer.AddSpecialTokens(System.Collections.Generic.Dictionary{System.String,System.Int32},Microsoft.ML.Tokenizers.Vec{System.String}@)">
            Add the provided special tokens to the initial vocabulary
        </member>
        <member name="M:Microsoft.ML.Tokenizers.BpeTrainer.TokenizeWords(System.Collections.Generic.Dictionary{System.String,System.Int32},System.Collections.Generic.Dictionary{System.String,System.Int32},Microsoft.ML.Tokenizers.Vec{System.String}@)">
            Tokenize words and add sub-words to the vocabulary when relevant
        </member>
        <member name="M:Microsoft.ML.Tokenizers.BpeTrainer.Feed(System.Collections.Generic.IEnumerable{System.String},System.Func{System.String,System.Collections.Generic.IEnumerable{System.String}})">
            <summary>
            Process the input sequences and feed the result to the model.
            </summary>
            <param name="sequences">The list of sequences to feed the trainer.</param>
            <param name="process">Optional process callback for reporting the training progress update.</param>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.BpeTrainer.Train(Microsoft.ML.Tokenizers.Model)">
            <summary>
            Perform the actual training and update the input model with the new vocabularies and merges data.
            </summary>
            <param name="model">The model to train. This has to be BpeModel to work with BpeTrainer.</param>
            <returns>The list of the added tokens.</returns>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.EnglishRoberta">
            <summary>
            Represent the Byte Pair Encoding model.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.#ctor(System.String,System.String,System.String)">
            <summary>
            Construct tokenizer object to use with the English Robert model.
            </summary>
            <param name="vocabularyPath">The JSON file path containing the dictionary of string keys and their ids.</param>
            <param name="mergePath">The file path containing the tokens's pairs list.</param>
            <param name="highestOccurrenceMappingPath">Remap the original GPT-2 model Ids to high occurrence ranks and values.</param>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.#ctor(System.IO.Stream,System.IO.Stream,System.IO.Stream)">
            <summary>
            Construct tokenizer object to use with the English Robert model.
            </summary>
            <param name="vocabularyStream">The stream of a JSON file containing the dictionary of string keys and their ids.</param>
            <param name="mergeStream">The stream of a file containing the tokens's pairs list.</param>
            <param name="highestOccurrenceMappingStream">Remap the original GPT-2 model Ids to high occurrence ranks and values.</param>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.GetVocab">
            <summary>
            Gets the dictionary mapping tokens to Ids.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.GetVocabSize">
            <summary>
            Gets the dictionary size that map tokens to Ids.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.IdToToken(System.Int32,System.Boolean)">
            <summary>
            Map the tokenized Id to the token.
            </summary>
            <param name="id">The Id to map to the token.</param>
            <param name="skipSpecialTokens">Indicate if want to skip the special tokens during the decoding.</param>
            <returns>The mapped token of the Id.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.Save(System.String,System.String)">
            <summary>
            Save the model data into the vocabulary, merges, and occurrence mapping files.
            </summary>
            <param name="path">The file system path to store the generated files at.</param>
            <param name="prefix">Optional prefix for the generated file names.</param>
            <returns>The list of all saved files.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.Tokenize(System.String)">
            <summary>
            Tokenize a sequence string to a list of tokens.
            </summary>
            <param name="sequence">The sequence to tokenize.</param>
            <returns>The list of tokens generated from the sequence tokenization.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.TokenToId(System.String)">
            <summary>
            Map the token to tokenized Id.
            </summary>
            <param name="token">The token to map to the Id.</param>
            <returns>The mapped Id of the token.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.GetTrainer">
            <summary>
            Gets a trainer object to use in training the model and generate the vocabulary and merges data.
            </summary>
            <remarks>
            This tokenizer doesn't support training so this method will return null. Consider using Bpe.GetTrainer() for training.
            </remarks>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.IdsToOccurrenceRanks(System.Collections.Generic.IReadOnlyList{System.Int32})">
            <summary>
            Convert a list of tokens Ids to highest occurrence rankings.
            </summary>
            <param name="ids">The Ids list to map to the high occurrence rank.</param>
            <returns>The list of ranks mapped from the list of Ids.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.IdsToOccurrenceValues(System.Collections.Generic.IReadOnlyList{System.Int32})">
            <summary>
            Convert a list of tokens Ids to highest occurrence values.
            </summary>
            <param name="ids">The Ids list to map to the high occurrence values.</param>
            <returns>The list of occurrence values mapped from the list of Ids.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.OccurrenceRanksIds(System.Collections.Generic.IReadOnlyList{System.Int32})">
            <summary>
            Convert a list of highest occurrence rankings to tokens Ids list .
            </summary>
            <param name="ranks">The high occurrence ranks list to map to the Ids list.</param>
            <returns>The list of Ids mapped from the list of ranks.</returns>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.EnglishRoberta.PadIndex">
            <summary>
            Gets the index of the pad symbol inside the symbols list.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.EnglishRoberta.SymbolsCount">
            <summary>
            Gets the symbols list length.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.AddMaskSymbol(System.String)">
            <summary>
            Add the mask symbol to the symbols list.
            </summary>
            <param name="mask">The mask symbol.</param>
            <returns>The index of the mask symbol in the symbols list.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.GetByteToUnicode(System.Collections.Generic.IReadOnlyDictionary{System.Char,System.Char}@)">
            <summary>
            Returns list of utf-8 bytes and a corresponding list of unicode chars.
            This mapping is to make unseen characters (such as control characters) displayable.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.BpeToken(System.Span{System.Char},System.Span{System.Int32})">
            <summary>
            Encode a token into BPE-ed sub-tokens. E.g., "playing" into ["play", "ing"].
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.EnglishRoberta.WordToPairs(System.Collections.Generic.IReadOnlyList{System.String})">
            <summary>
            Extract element pairs in an aggregating word. E.g. [p, l, ay] into [(p,l), (l,ay)].
            If word contains 0 or 1 element, an empty HashSet will be returned.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.HighestOccurrenceMapping">
            <summary>
            HighestOccurrenceMapping maps the GPT-2 vocabulary Id to highest occurrence value came from dict.txt file
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.HighestOccurrenceMapping.#ctor(System.String,System.String,System.String,System.String,System.String[])">
            <exception cref="T:System.ArgumentNullException">Any of `pad`, `eos`, `unk` and `bos` is `null`.</exception>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.HighestOccurrenceMapping.Item(System.Int32)">
            <exception cref="T:System.ArgumentOutOfRangeException">`idx` is negative.</exception>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.HighestOccurrenceMapping.IndexOf(System.Int32)">
            <exception cref="T:System.ArgumentNullException">`symbol` is `null`.</exception>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.HighestOccurrenceMapping.Load(System.IO.Stream)">
            <summary>
            Loads the mapping from a text file with the format:
                13 850314647
                262 800385005
                11 800251374
                284 432911125
                ...
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.HighestOccurrenceMapping.AddFromStream(System.IO.Stream)">
            <summary>
            Loads a pre-existing vocabulary from a text stream and adds its symbols to this instance.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.Model">
            <summary>
            Represents a model used during Tokenization (like BPE or Word Piece or Unigram).
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Model.Tokenize(System.String)">
            <summary>
            Tokenize a sequence string to a list of tokens.
            </summary>
            <param name="sequence">The sequence to tokenize.</param>
            <returns>The list of tokens generated from the sequence tokenization.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Model.TokenToId(System.String)">
            <summary>
            Map the token to tokenized Id.
            </summary>
            <param name="token">The token to map to the Id.</param>
            <returns>The mapped Id of the token.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Model.IdToToken(System.Int32,System.Boolean)">
            <summary>
            Map the tokenized Id to the token.
            </summary>
            <param name="id">The Id to map to the token.</param>
            <param name="skipSpecialTokens">Indicate if want to skip the special tokens during the decoding.</param>
            <returns>The mapped token of the Id.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Model.GetVocab">
            <summary>
            Gets the dictionary mapping tokens to Ids.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Model.GetVocabSize">
            <summary>
            Gets the dictionary size that map tokens to Ids.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Model.Save(System.String,System.String)">
            <summary>
            Save the model data into the vocabulary and merges files.
            </summary>
            <param name="path">The file system path to store the generated files at.</param>
            <param name="prefix">Optional prefix for the generated file names.</param>
            <returns>The list of all saved files.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Model.GetTrainer">
            <summary>
            Gets a trainer object to use in training the model.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.ProgressState">
            <summary>
            Represent the state of the reported progress.
            </summary>
        </member>
        <member name="F:Microsoft.ML.Tokenizers.ProgressState.Start">
            <summary>
            The progress is started. The reported value in the Progress structure will have the max number progressing toward.
            </summary>
        </member>
        <member name="F:Microsoft.ML.Tokenizers.ProgressState.End">
            <summary>
            The progress is ended. The reported value in the Progress structure will have the final max processed number.
            </summary>
        </member>
        <member name="F:Microsoft.ML.Tokenizers.ProgressState.Increment">
            <summary>
            The progress is incremented. The reported value in increment value in the progress.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Progress.#ctor(Microsoft.ML.Tokenizers.ProgressState,System.String,System.Int32)">
            <summary>
            Construct the Progress object using the progress state, message and the value.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Progress.State">
            <summary>
            The progress state.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Progress.Message">
            <summary>
            The message of the progress.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Progress.Value">
            <summary>
            The Value of the progress.
            </summary>
            <remarks>
            The value is the max number the progress can reach if the progress state is `Start`.
            The value is the max number the progress reached if the progress state is `End`.
            The value is the incremented value in the progress if the progress state is `Increment`.
            </remarks>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Symbol.MergeWith(Microsoft.ML.Tokenizers.Symbol@,System.Int32)">
            Merges the current Symbol with the other one.
            In order to update prev/next, we consider Self to be the Symbol on the left,
            and other to be the next one on the right.
        </member>
        <member name="T:Microsoft.ML.Tokenizers.Trainer">
            <summary>
            A `Trainer` has the responsibility to train a model. We feed it with lines/sentences
            and then it can train the given `Model`.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Trainer.Progress">
            <summary>
            Set when need to report the progress during the training.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Trainer.Train(Microsoft.ML.Tokenizers.Model)">
            <summary>
            Perform the actual training and update the input model with the new vocabularies and merges data.
            </summary>
            <param name="model">The model to train.</param>
            <returns>Special tokens to be added directly to the tokenizer along with the model.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Trainer.Feed(System.Collections.Generic.IEnumerable{System.String},System.Func{System.String,System.Collections.Generic.IEnumerable{System.String}})">
            <summary>
            Process the input sequences and feed the result to the model.
            </summary>
            <param name="sequences">The list of sequences to feed the trainer.</param>
            <param name="process">Optional process callback for reporting the training progress update.</param>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.LowerCaseNormalizer">
            <summary>
            Normalize the string to lowercase form before processing it with the tokenizer.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.LowerCaseNormalizer.#ctor">
            <summary>
            Creates a LowerCaseNormalizer object.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.LowerCaseNormalizer.Normalize(System.String)">
            <summary>
            Lowercase the original string.
            </summary>
            <param name="original">The original string to normalize to lowercase form.</param>
            <returns>The lower-cased normalized string.</returns>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.NormalizedString">
            <summary>
            Contains the normalized string and the mapping to the original string.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.NormalizedString.#ctor(System.String,System.String,System.Int32[],System.Boolean)">
            <summary>
            Create NormalizedString object containing the normalization of the original string and the mapping
            between the original and the normalized string.
            </summary>
            <param name="original">The original string before normalization.</param>
            <param name="normalizedString">The normalized string.</param>
            <param name="mapping">The mapping between the normalized string and the original string.</param>
            <param name="isOneToOneMapping">Indicate whether the mapping is one-to-one.</param>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.NormalizedString.Original">
            <summary>
            Gets the original string before the normalization.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.NormalizedString.Normalized">
            <summary>
            Gets the normalized string.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.NormalizedString.NormalizedToOriginalMapping">
            <summary>
            Gets the mapping between the normalized string and the original string.
            </summary>
            <remarks>
            The mapping can be null if IsOneToOneMapping is true or if the normalization doesn't support the mapping.
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.NormalizedString.IsOneToOneMapping">
            <summary>
            Gets whether the normalization between the normalized string and the original string is one-to-one.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.NormalizedString.CanMapToOriginal">
            <summary>
            Gets whether can map the normalized string the original string.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.Normalizer">
            <summary>
            Normalize the string before processing it with the tokenizer.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Normalizer.Normalize(System.String)">
            <summary>
            Process the original string to modify it and obtain a normalized string.
            </summary>
            <param name="original">The original string to normalize.</param>
            <returns>The normalized string along with the mapping to the original string.</returns>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.UpperCaseNormalizer">
            <summary>
            Normalize the string to uppercase form before processing it with the tokenizer.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.UpperCaseNormalizer.#ctor">
            <summary>
            Creates a UpperCaseNormalizer object.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.UpperCaseNormalizer.Normalize(System.String)">
            <summary>
            Uppercase the original string.
            </summary>
            <param name="original">The original string to normalize to uppercase form.</param>
            <returns>The upper-cased normalized string.</returns>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.Split">
            <summary>
            This Split contains the underlying split token as well as its offsets
            in the original string. These offsets are in the `original` referential.
            It also contains any `Token` associated to the current split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Split.TokenString">
            <summary>
            Gets the underlying split token. Each SubString is represented by a token
            and in the end we might be carrying a lot of SubString representing various parts of the
            original input string.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Split.Offset">
            <summary>
            Returns the offset mapping to the original string
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Split.#ctor(System.String,System.ValueTuple{System.Int32,System.Int32})">
            <summary>
            create a Split object using the token and the offset
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Split.Equals(Microsoft.ML.Tokenizers.Split)">
            <summary>
            Indicates whether the current Split object is equal to another Split object.
            </summary>
            <param name="other">The Split object to compare with the current object.</param>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.PreTokenizer">
            <summary>
            Base class for all pre-tokenizers classes.
            The PreTokenizer is in charge of doing the pre-segmentation step.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.PreTokenizer.PreTokenize(System.String)">
            <summary>
            Splits the given string in multiple substrings at the word boundary, keeping track of the offsets of said substrings from the original string.
            </summary>
            <param name="sentence">The string to split into tokens.</param>
            <returns>The list of the splits containing the tokens and the token's offsets to the original string.</returns>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.RobertaPreTokenizer">
            <summary>
            The pre-tokenizer for Roberta English tokenizer.
            </summary>
        </member>
        <member name="F:Microsoft.ML.Tokenizers.RobertaPreTokenizer.Instance">
            <summary>
            Gets a singleton instance of the Roberta pre-tokenizer..
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.RobertaPreTokenizer.PreTokenize(System.String)">
            <summary>
            Splits the given string in multiple substrings at the word boundary, keeping track of the offsets of said substrings from the original string.
            </summary>
            <param name="sentence">The string to split into tokens.</param>
            <returns>The list of the splits containing the tokens and the token's offsets to the original string.</returns>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.WhiteSpace">
            <summary>
            The pre-tokenizer which split the text at the word boundary.
            The word is a set of alphabet, numeric, and underscore characters.
            </summary>
        </member>
        <member name="F:Microsoft.ML.Tokenizers.WhiteSpace.Instance">
            <summary>
            Gets a singleton instance of the WhiteSpace pre-tokenizer..
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.WhiteSpace.PreTokenize(System.String)">
            <summary>
            Splits the given string in multiple substrings at the word boundary, keeping track of the offsets of said substrings from the original string.
            </summary>
            <param name="sentence">The string to split into tokens.</param>
            <returns>The list of the splits containing the tokens and the token's offsets to the original string.</returns>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.Token">
            <summary>
            Represent the token produced from the tokenization process containing the token substring,
            the id associated to the token substring, and the offset mapping to the original string.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Token.Id">
            <summary>
            Gets or sets the Id value associated to the token.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Token.Value">
            <summary>
            Gets or sets the token string value.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Token.Offset">
            <summary>
            Gets or sets the offset mapping to the original string.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Token.#ctor(System.Int32,System.String,System.ValueTuple{System.Int32,System.Int32})">
            <summary>
            Construct a new Token object using the token value, Id, and the offset mapping to the original string.
            </summary>
            <param name="id">The Id value associated to the token.</param>
            <param name="value">The token string value.</param>
            <param name="offset">The offset mapping to the original string.</param>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.Tokenizer">
            <summary>
            A Tokenizer works as a pipeline. It processes some raw text as input and outputs a TokenizerResult object.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Tokenizer.#ctor(Microsoft.ML.Tokenizers.Model,Microsoft.ML.Tokenizers.PreTokenizer,Microsoft.ML.Tokenizers.Normalizer)">
            <summary>
            Create a new Tokenizer object.
            </summary>
            <param name="model">The Model in use by the Tokenizer.</param>
            <param name="preTokenizer">The optional PreTokenizer in use by the Tokenizer. WhiteSpace PreTokenizer will be used if this parameter is null.</param>
            <param name="normalizer">The optional Normalizer in use by the Tokenizer.</param>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Tokenizer.Model">
            <summary>
            Gets the Model in use by the Tokenizer.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Tokenizer.PreTokenizer">
            <summary>
            Gets or sets the PreTokenizer used by the Tokenizer.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Tokenizer.Normalizer">
            <summary>
            Gets or sets the Normalizer in use by the Tokenizer.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.Tokenizer.Decoder">
            <summary>
            Gets or sets the Decoder in use by the Tokenizer.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Tokenizer.Encode(System.String)">
            <summary>
            Encodes input text to object has the tokens list, tokens Ids, tokens offset mapping.
            </summary>
            <param name="sequence">The text to tokenize.</param>
            <returns>The tokenization result includes the tokens list, tokens Ids, tokens offset mapping.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Tokenizer.Decode(System.Int32,System.Boolean)">
            <summary>
            Decodes the Id to the mapped token.
            </summary>
            <param name="id">The id to map to the token.</param>
            <param name="skipSpecialTokens">Indicate if want to skip the special tokens during the decoding.</param>
            <returns>The decoded string or null if there is no token mapped to the input id.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Tokenizer.Decode(System.Collections.Generic.IEnumerable{System.Int32},System.Boolean)">
            <summary>
            Decode the given ids, back to a String.
            </summary>
            <param name="ids">The list of ids that we want to decode.</param>
            <param name="skipSpecialTokens">Whether the special tokens should be removed from the decoded string.</param>
            <returns>The decoded string.</returns>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.Tokenizer.TrainFromFiles(Microsoft.ML.Tokenizers.Trainer,Microsoft.ML.Tokenizers.ReportProgress,System.String[])">
            <summary>
            Train the tokenizer model using input files.
            </summary>
            <param name="trainer">An optional trainer that should be used to train our Model.</param>
            <param name="progress">Optional progress callback to report the training progress.</param>
            <param name="files">A list of the files that we should use for training.</param>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.TokenizerDecoder">
            <summary>
            A Decoder has the responsibility to merge the given list of tokens in a string.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.TokenizerDecoder.Decode(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Decode by joining all the tokens to a string.
            </summary>
            <param name="tokens">The list of tokens to merge.</param>
            <returns>The string containing all merged tokens.</returns>
        </member>
        <member name="T:Microsoft.ML.Tokenizers.TokenizerResult">
            <summary>
            The Encoding represents the output of a Tokenizer.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Tokenizers.TokenizerResult.#ctor(System.String,System.String,System.Collections.Generic.IReadOnlyList{Microsoft.ML.Tokenizers.Split},System.Boolean)">
            <summary>
            Create a new object of the TokenizerResult object.
            </summary>
            <param name="originalString">The list of tokens to merge.</param>
            <param name="normalizedString">The list of tokens to merge.</param>
            <param name="splits">The list of tokens to merge.</param>
            <param name="offsetsMappedToOriginalString">Indicate whether the offsets is mapped to the original string or the normalized string.</param>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.TokenizerResult.OriginalString">
            <summary>
            Gets the original tokenized string.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.TokenizerResult.NormalizedString">
            <summary>
            Gets the normalized form of the original string.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.TokenizerResult.OffsetsMappedToOriginalString">
            <summary>
            Gets the normalized form of the original string.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.TokenizerResult.Ids">
            <summary>
            Gets list of the tokens Ids.
            The Ids are the main input to a Language Model. They are the token indices, the numerical representations that a LM understands.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.TokenizerResult.Tokens">
            <summary>
            Gets the generated tokens. They are the string representation of the Ids.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Tokenizers.TokenizerResult.Offsets">
            <summary>
            Gets The list of offsets. These offsets let’s you slice the input string, and thus retrieve
            the original part that led to producing the corresponding token.
            </summary>
        </member>
    </members>
</doc>
